"""
ç”Ÿæˆé›†æˆæ¨¡å‹åœ¨å¹³è¡¡é˜ˆå€¼ä¸‹çš„æ··æ·†çŸ©é˜µå›¾ç‰‡
ä¸“æ³¨äºç”Ÿæˆæœªå½’ä¸€åŒ–çš„æ··æ·†çŸ©é˜µå¯è§†åŒ–
"""

import pandas as pd
import numpy as np
import pickle
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

def load_model_and_preprocessing():
    """åŠ è½½é›†æˆæ¨¡å‹å’Œé¢„å¤„ç†ç»„ä»¶"""
    print("=== åŠ è½½æ¨¡å‹å’Œé¢„å¤„ç†ç»„ä»¶ ===")
    
    try:
        # åŠ è½½é›†æˆæ¨¡å‹
        with open('Model/ensemble_model.pkl', 'rb') as f:
            ensemble_model = pickle.load(f)
        print("âœ… é›†æˆæ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # åŠ è½½é¢„å¤„ç†ç»„ä»¶
        with open('Model/ensemble_preprocessing.pkl', 'rb') as f:
            preprocessing_components = pickle.load(f)
        print("âœ… é¢„å¤„ç†ç»„ä»¶åŠ è½½æˆåŠŸ")
        
        # é»˜è®¤å¹³è¡¡é˜ˆå€¼
        balanced_threshold = 0.5
        print(f"ä½¿ç”¨å¹³è¡¡é˜ˆå€¼: {balanced_threshold}")
        
        return ensemble_model, preprocessing_components, balanced_threshold
        
    except FileNotFoundError as e:
        print(f"âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {e}")
        return None, None, None
    except Exception as e:
        print(f"âŒ åŠ è½½æ¨¡å‹æ—¶å‡ºé”™: {e}")
        return None, None, None

def advanced_feature_engineering(df):
    """é«˜çº§ç‰¹å¾å·¥ç¨‹ï¼ˆä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ï¼‰"""
    print("è¿›è¡Œç‰¹å¾å·¥ç¨‹...")
    
    df_eng = df.copy()
    
    # 1. Zå€¼ç›¸å…³ç‰¹å¾
    z_cols = ['XæŸ“è‰²ä½“çš„Zå€¼ç»å¯¹å€¼', '21å·æŸ“è‰²ä½“çš„Zå€¼ç»å¯¹å€¼', '18å·æŸ“è‰²ä½“çš„Zå€¼ç»å¯¹å€¼', '13å·æŸ“è‰²ä½“çš„Zå€¼ç»å¯¹å€¼']
    
    # åŸºç¡€ç»Ÿè®¡ç‰¹å¾
    df_eng['æœ€å¤§Zå€¼'] = df_eng[z_cols].max(axis=1)
    df_eng['å¹³å‡Zå€¼'] = df_eng[z_cols].mean(axis=1)
    df_eng['Zå€¼æ ‡å‡†å·®'] = df_eng[z_cols].std(axis=1)
    df_eng['Zå€¼å˜å¼‚ç³»æ•°'] = df_eng[z_cols].std(axis=1) / (df_eng[z_cols].mean(axis=1) + 1e-8)
    df_eng['Zå€¼èŒƒå›´'] = df_eng[z_cols].max(axis=1) - df_eng[z_cols].min(axis=1)
    
    # å¼‚å¸¸æŒ‡æ ‡ï¼ˆå¤šé˜ˆå€¼ï¼‰
    thresholds = [1.0, 1.5, 2.0, 2.5, 3.0, 3.5]
    for threshold in thresholds:
        df_eng[f'Zå€¼è¶…{threshold}_count'] = (df_eng[z_cols] > threshold).sum(axis=1)
        df_eng[f'Zå€¼è¶…{threshold}_æ¯”ä¾‹'] = df_eng[f'Zå€¼è¶…{threshold}_count'] / len(z_cols)
    
    # ç‰¹å®šæŸ“è‰²ä½“ç»„åˆå¼‚å¸¸
    df_eng['å¸¸æŸ“è‰²ä½“å¼‚å¸¸'] = (
        (df_eng['21å·æŸ“è‰²ä½“çš„Zå€¼ç»å¯¹å€¼'] > 2.0) |
        (df_eng['18å·æŸ“è‰²ä½“çš„Zå€¼ç»å¯¹å€¼'] > 2.0) |
        (df_eng['13å·æŸ“è‰²ä½“çš„Zå€¼ç»å¯¹å€¼'] > 2.0)
    ).astype(int)
    
    df_eng['XæŸ“è‰²ä½“å¼‚å¸¸'] = (df_eng['XæŸ“è‰²ä½“çš„Zå€¼ç»å¯¹å€¼'] > 2.0).astype(int)
    
    # 2. GCå«é‡ç‰¹å¾
    gc_cols = ['13å·æŸ“è‰²ä½“çš„GCå«é‡', '18å·æŸ“è‰²ä½“çš„GCå«é‡', '21å·æŸ“è‰²ä½“çš„GCå«é‡']
    
    df_eng['å¹³å‡GCå«é‡'] = df_eng[gc_cols].mean(axis=1)
    df_eng['GCæ ‡å‡†å·®'] = df_eng[gc_cols].std(axis=1)
    df_eng['GCåå·®'] = abs(df_eng['GCå«é‡'] - df_eng['å¹³å‡GCå«é‡'])
    
    # GCå«é‡å¼‚å¸¸æŒ‡æ ‡
    df_eng['GCå«é‡å¼‚å¸¸'] = (
        (df_eng['GCåå·®'] > df_eng['GCåå·®'].quantile(0.9)) |
        (df_eng['GCæ ‡å‡†å·®'] > df_eng['GCæ ‡å‡†å·®'].quantile(0.9))
    ).astype(int)
    
    # 3. XæŸ“è‰²ä½“ç‰¹å¼‚æ€§ç‰¹å¾
    df_eng['XæŸ“è‰²ä½“æµ“åº¦å¼‚å¸¸'] = (
        (df_eng['XæŸ“è‰²ä½“æµ“åº¦'] < 0.45) | (df_eng['XæŸ“è‰²ä½“æµ“åº¦'] > 0.55)
    ).astype(int)
    df_eng['XæŸ“è‰²ä½“æµ“åº¦åå·®'] = abs(df_eng['XæŸ“è‰²ä½“æµ“åº¦'] - 0.5)
    
    # 4. æµ‹åºè´¨é‡ç‰¹å¾
    df_eng['æµ‹åºæ·±åº¦å……è¶³'] = (df_eng['å”¯ä¸€æ¯”å¯¹çš„è¯»æ®µæ•°'] > 3000000).astype(int)
    df_eng['æ¯”å¯¹è´¨é‡è‰¯å¥½'] = (df_eng['åœ¨å‚è€ƒåŸºå› ç»„ä¸Šæ¯”å¯¹çš„æ¯”ä¾‹'] > 0.75).astype(int)
    df_eng['é‡å¤ç‡ä½'] = (df_eng['é‡å¤è¯»æ®µçš„æ¯”ä¾‹'] < 0.2).astype(int)
    
    # 5. ä¸´åºŠé£é™©å› å­
    df_eng['é«˜é¾„äº§å¦‡'] = (df_eng['å¹´é¾„'] >= 35).astype(int)
    df_eng['æé«˜é¾„äº§å¦‡'] = (df_eng['å¹´é¾„'] >= 40).astype(int)
    df_eng['BMIè¿‡é«˜'] = (df_eng['å­•å¦‡BMI'] > 30).astype(int)
    df_eng['BMIè‚¥èƒ–'] = (df_eng['å­•å¦‡BMI'] > 35).astype(int)
    
    # 6. äº¤äº’ç‰¹å¾
    df_eng['å¹´é¾„_Zå€¼äº¤äº’'] = df_eng['å¹´é¾„'] * df_eng['æœ€å¤§Zå€¼']
    df_eng['BMI_Zå€¼äº¤äº’'] = df_eng['å­•å¦‡BMI'] * df_eng['æœ€å¤§Zå€¼']
    df_eng['GC_Zå€¼äº¤äº’'] = df_eng['GCåå·®'] * df_eng['æœ€å¤§Zå€¼']
    
    # 7. å¤šé‡é£é™©è¯„åˆ†
    df_eng['ä¿å®ˆé£é™©è¯„åˆ†'] = (
        df_eng['æœ€å¤§Zå€¼'] * 0.4 +
        df_eng['Zå€¼è¶…2.5_count'] * 0.3 +
        df_eng['XæŸ“è‰²ä½“æµ“åº¦åå·®'] * 0.2 +
        df_eng['é«˜é¾„äº§å¦‡'] * 0.1
    )
    
    df_eng['å¹³è¡¡é£é™©è¯„åˆ†'] = (
        df_eng['æœ€å¤§Zå€¼'] * 0.3 +
        df_eng['Zå€¼è¶…2.0_count'] * 0.25 +
        df_eng['Zå€¼è¶…1.5_count'] * 0.2 +
        df_eng['GCåå·®'] * 0.15 +
        df_eng['é«˜é¾„äº§å¦‡'] * 0.1
    )
    
    df_eng['æ•æ„Ÿé£é™©è¯„åˆ†'] = (
        df_eng['æœ€å¤§Zå€¼'] * 0.25 +
        df_eng['Zå€¼è¶…1.5_count'] * 0.3 +
        df_eng['Zå€¼è¶…1.0_count'] * 0.25 +
        df_eng['GCåå·®'] * 0.1 +
        df_eng['é«˜é¾„äº§å¦‡'] * 0.05 +
        df_eng['BMIè¿‡é«˜'] * 0.05
    )
    
    # 8. å¤åˆå¼‚å¸¸æŒ‡æ ‡
    df_eng['å¤šé‡å¼‚å¸¸æŒ‡æ ‡'] = (
        df_eng['Zå€¼è¶…2.0_count'] +
        df_eng['GCå«é‡å¼‚å¸¸'] +
        df_eng['XæŸ“è‰²ä½“æµ“åº¦å¼‚å¸¸'] +
        df_eng['é«˜é¾„äº§å¦‡']
    )
    
    print(f"ç‰¹å¾å·¥ç¨‹å®Œæˆï¼Œç‰¹å¾æ•°é‡: {df_eng.shape[1]}")
    return df_eng

def preprocess_features(df, preprocessing_components):
    """ç‰¹å¾é¢„å¤„ç†"""
    print("=== ç‰¹å¾é¢„å¤„ç† ===")
    
    # è·å–æ•°å€¼ç‰¹å¾
    numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()
    exclude_cols = ['åºå·', 'is_abnormal']
    feature_cols = [col for col in numeric_features if col not in exclude_cols]
    
    # ç¡®ä¿ä½¿ç”¨è®­ç»ƒæ—¶ç›¸åŒçš„ç‰¹å¾
    if 'feature_cols' in preprocessing_components:
        train_feature_cols = preprocessing_components['feature_cols']
        available_features = [col for col in train_feature_cols if col in df.columns]
        missing_features = [col for col in train_feature_cols if col not in df.columns]
        
        if missing_features:
            print(f"è­¦å‘Š: ç¼ºå°‘è®­ç»ƒæ—¶çš„ç‰¹å¾: {missing_features}")
        
        feature_cols = available_features
    
    print(f"ä½¿ç”¨ç‰¹å¾æ•°é‡: {len(feature_cols)}")
    
    X = df[feature_cols]
    
    # 1. ç¼ºå¤±å€¼å¤„ç†
    imputer = preprocessing_components['imputer']
    X_imputed = imputer.transform(X)
    
    # 2. ç‰¹å¾é€‰æ‹©
    selector = preprocessing_components['selector']
    X_selected = selector.transform(X_imputed)
    
    # 3. æ ‡å‡†åŒ–
    scaler = preprocessing_components['scaler']
    X_scaled = scaler.transform(X_selected)
    
    print(f"é¢„å¤„ç†åç‰¹å¾ç»´åº¦: {X_scaled.shape}")
    return X_scaled

def generate_confusion_matrix_plot(y_true, y_pred, balanced_threshold):
    """ç”Ÿæˆæ··æ·†çŸ©é˜µå›¾ç‰‡"""
    print("=== ç”Ÿæˆæ··æ·†çŸ©é˜µå›¾ç‰‡ ===")
    
    # è®¡ç®—æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_true, y_pred)
    
    # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred, zero_division=0)
    recall = recall_score(y_true, y_pred, zero_division=0)
    f1 = f1_score(y_true, y_pred, zero_division=0)
    
    # åˆ›å»ºå›¾ç‰‡
    plt.figure(figsize=(10, 8))
    
    # ç»˜åˆ¶æ··æ·†çŸ©é˜µçƒ­åŠ›å›¾
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
               xticklabels=['æ­£å¸¸', 'å¼‚å¸¸'], 
               yticklabels=['æ­£å¸¸', 'å¼‚å¸¸'],
               cbar_kws={'label': 'æ ·æœ¬æ•°é‡'})
    
    plt.title(f'æ··æ·†çŸ©é˜µ (å¹³è¡¡é˜ˆå€¼: {balanced_threshold:.3f})', fontsize=16, fontweight='bold', pad=20)
    plt.xlabel('é¢„æµ‹æ ‡ç­¾', fontsize=14)
    plt.ylabel('çœŸå®æ ‡ç­¾', fontsize=14)
    
    # åœ¨å›¾çš„å³ä¾§æ·»åŠ æ€§èƒ½æŒ‡æ ‡
    metrics_text = f"""æ€§èƒ½æŒ‡æ ‡:
Accuracy: {accuracy:.4f}
Precision: {precision:.4f}
Recall: {recall:.4f}
F1-Score: {f1:.4f}

çœŸé˜´æ€§(TN): {cm[0,0]}
å‡é˜³æ€§(FP): {cm[0,1]}
å‡é˜´æ€§(FN): {cm[1,0]}
çœŸé˜³æ€§(TP): {cm[1,1]}"""
    
    plt.figtext(0.75, 0.6, metrics_text, fontsize=12,
               bbox=dict(boxstyle="round,pad=0.5", facecolor="lightblue", alpha=0.8))
    
    plt.tight_layout()
    plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # æ‰“å°è¯¦ç»†ç»“æœ
    print("\nğŸ“Š æ··æ·†çŸ©é˜µç»“æœ:")
    print(f"çœŸé˜´æ€§ (TN): {cm[0,0]} - æ­£ç¡®é¢„æµ‹ä¸ºæ­£å¸¸çš„æ ·æœ¬")
    print(f"å‡é˜³æ€§ (FP): {cm[0,1]} - é”™è¯¯é¢„æµ‹ä¸ºå¼‚å¸¸çš„æ­£å¸¸æ ·æœ¬") 
    print(f"å‡é˜´æ€§ (FN): {cm[1,0]} - é”™è¯¯é¢„æµ‹ä¸ºæ­£å¸¸çš„å¼‚å¸¸æ ·æœ¬")
    print(f"çœŸé˜³æ€§ (TP): {cm[1,1]} - æ­£ç¡®é¢„æµ‹ä¸ºå¼‚å¸¸çš„æ ·æœ¬")
    
    print("\nğŸ“ˆ æ€§èƒ½æŒ‡æ ‡:")
    print(f"å‡†ç¡®ç‡ (Accuracy): {accuracy:.4f}")
    print(f"ç²¾ç¡®ç‡ (Precision): {precision:.4f}")
    print(f"å¬å›ç‡ (Recall): {recall:.4f}")
    print(f"F1åˆ†æ•°: {f1:.4f}")
    
    return cm

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ å¼€å§‹ç”Ÿæˆæ··æ·†çŸ©é˜µ")
    print("="*50)
    
    # 1. åŠ è½½æ¨¡å‹
    ensemble_model, preprocessing_components, balanced_threshold = load_model_and_preprocessing()
    if ensemble_model is None:
        return
    
    # 2. åŠ è½½æ•°æ®
    print("\n=== åŠ è½½æ•°æ® ===")
    df = pd.read_csv('clean_girls_data_Q4.csv')
    print(f"æ•°æ®é›†å¤§å°: {df.shape}")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰çœŸå®æ ‡ç­¾
    if 'is_abnormal' not in df.columns:
        print("âŒ æ•°æ®ä¸­æ²¡æœ‰ 'is_abnormal' åˆ—ï¼Œæ— æ³•ç”Ÿæˆæ··æ·†çŸ©é˜µ")
        return
    
    y_true = np.array(df['is_abnormal'].astype(int))
    total_abnormal = int(y_true.sum())
    abnormal_rate = float(y_true.mean())
    print(f"çœŸå®å¼‚å¸¸æ ·æœ¬: {total_abnormal} / {len(y_true)} ({abnormal_rate:.1%})")
    
    # 3. ç‰¹å¾å·¥ç¨‹
    processed_df = advanced_feature_engineering(df)
    
    # 4. ç‰¹å¾é¢„å¤„ç†
    X = preprocess_features(processed_df, preprocessing_components)
    
    # 5. ç”Ÿæˆé¢„æµ‹
    print(f"\n=== ä½¿ç”¨å¹³è¡¡é˜ˆå€¼ {balanced_threshold:.3f} è¿›è¡Œé¢„æµ‹ ===")
    probabilities = ensemble_model.predict_proba(X)
    abnormal_probabilities = probabilities[:, 1]
    
    # ä½¿ç”¨å¹³è¡¡é˜ˆå€¼ç”Ÿæˆé¢„æµ‹
    y_pred = (abnormal_probabilities > balanced_threshold).astype(int)
    
    positive_count = y_pred.sum()
    positive_rate = positive_count / len(y_pred)
    print(f"é¢„æµ‹ä¸ºé˜³æ€§çš„æ ·æœ¬æ•°: {positive_count} / {len(y_pred)} ({positive_rate:.1%})")
    
    # 6. ç”Ÿæˆæ··æ·†çŸ©é˜µå›¾ç‰‡
    generate_confusion_matrix_plot(y_true, y_pred, balanced_threshold)
    
    print("\nğŸ‰ æ··æ·†çŸ©é˜µç”Ÿæˆå®Œæˆï¼")
    print("ç”Ÿæˆçš„æ–‡ä»¶: confusion_matrix.png")

if __name__ == "__main__":
    main()
