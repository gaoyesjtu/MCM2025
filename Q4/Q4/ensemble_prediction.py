"""
ä½¿ç”¨é›†æˆæ¨¡å‹å¯¹æ–°æ•°æ®è¿›è¡Œé¢„æµ‹
å®Œæ•´çš„é¢„æµ‹æµæ°´çº¿ï¼ŒåŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹åŠ è½½ã€é¢„æµ‹å’Œç»“æœåˆ†æ
"""

import pandas as pd
import numpy as np
import pickle
import json
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

class EnsemblePredictionSystem:
    """é›†æˆæ¨¡å‹é¢„æµ‹ç³»ç»Ÿ"""
    
    def __init__(self):
        self.ensemble_model = None
        self.preprocessing_components = None
        self.model_info = None
        self.predictions = None
        self.data = None
        
    def load_ensemble_model(self):
        """åŠ è½½é›†æˆæ¨¡å‹å’Œé¢„å¤„ç†ç»„ä»¶"""
        print("=== åŠ è½½é›†æˆæ¨¡å‹å’Œé¢„å¤„ç†ç»„ä»¶ ===")
        
        try:
            # åŠ è½½é›†æˆæ¨¡å‹
            with open('ensemble_model.pkl', 'rb') as f:
                self.ensemble_model = pickle.load(f)
            print("âœ… é›†æˆæ¨¡å‹åŠ è½½æˆåŠŸ")
            
            # åŠ è½½é¢„å¤„ç†ç»„ä»¶
            with open('ensemble_preprocessing.pkl', 'rb') as f:
                self.preprocessing_components = pickle.load(f)
            print("âœ… é¢„å¤„ç†ç»„ä»¶åŠ è½½æˆåŠŸ")
            
            # åŠ è½½æ¨¡å‹ä¿¡æ¯
            with open('ensemble_model_info.json', 'r', encoding='utf-8') as f:
                self.model_info = json.load(f)
            print("âœ… æ¨¡å‹ä¿¡æ¯åŠ è½½æˆåŠŸ")
            
            print("\nğŸ“‹ æ¨¡å‹ä¿¡æ¯:")
            print(f"   é›†æˆç»„æˆ: {self.model_info['ensemble_composition']}")
            print(f"   æŠ•ç¥¨æ–¹å¼: {self.model_info['voting']}")
            print(f"   åˆ›å»ºæ—¶é—´: {self.model_info['creation_date']}")
            print(f"   é˜ˆå€¼è®¾ç½®: {self.model_info['optimal_thresholds']}")
            
            return True
            
        except FileNotFoundError as e:
            print(f"âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {e}")
            print("è¯·ç¡®ä¿å·²è¿è¡Œ create_ensemble_model.py ç”Ÿæˆæ¨¡å‹æ–‡ä»¶")
            return False
        except Exception as e:
            print(f"âŒ åŠ è½½æ¨¡å‹æ—¶å‡ºé”™: {e}")
            return False
    
    def load_and_preprocess_data(self, filepath='clean_girls_data_Q4.csv'):
        """åŠ è½½å¹¶é¢„å¤„ç†æ–°æ•°æ®"""
        print(f"\n=== åŠ è½½å’Œé¢„å¤„ç†æ•°æ®: {filepath} ===")
        
        # åŠ è½½æ•°æ®
        self.data = pd.read_csv(filepath)
        print(f"æ•°æ®é›†å¤§å°: {self.data.shape}")
        print(f"åˆ—å: {list(self.data.columns)}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰çœŸå®æ ‡ç­¾
        has_labels = 'is_abnormal' in self.data.columns
        if has_labels:
            print(f"åŒ…å«çœŸå®æ ‡ç­¾: å¼‚å¸¸æ ·æœ¬ {self.data['is_abnormal'].sum()} / {len(self.data)} = {self.data['is_abnormal'].mean():.1%}")
        else:
            print("æœªåŒ…å«çœŸå®æ ‡ç­¾ï¼Œå°†è¿›è¡Œçº¯é¢„æµ‹")
        
        # è¿›è¡Œç‰¹å¾å·¥ç¨‹ï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
        processed_data = self.advanced_feature_engineering(self.data)
        
        return processed_data, has_labels
    
    def advanced_feature_engineering(self, df):
        """é«˜çº§ç‰¹å¾å·¥ç¨‹ï¼ˆä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ï¼‰"""
        print("è¿›è¡Œç‰¹å¾å·¥ç¨‹...")
        
        df_eng = df.copy()
        
        # 1. Zå€¼ç›¸å…³ç‰¹å¾
        z_cols = ['XæŸ“è‰²ä½“çš„Zå€¼ç»å¯¹å€¼', '21å·æŸ“è‰²ä½“çš„Zå€¼ç»å¯¹å€¼', '18å·æŸ“è‰²ä½“çš„Zå€¼ç»å¯¹å€¼', '13å·æŸ“è‰²ä½“çš„Zå€¼ç»å¯¹å€¼']
        
        # åŸºç¡€ç»Ÿè®¡ç‰¹å¾
        df_eng['æœ€å¤§Zå€¼'] = df_eng[z_cols].max(axis=1)
        df_eng['å¹³å‡Zå€¼'] = df_eng[z_cols].mean(axis=1)
        df_eng['Zå€¼æ ‡å‡†å·®'] = df_eng[z_cols].std(axis=1)
        df_eng['Zå€¼å˜å¼‚ç³»æ•°'] = df_eng[z_cols].std(axis=1) / (df_eng[z_cols].mean(axis=1) + 1e-8)
        df_eng['Zå€¼èŒƒå›´'] = df_eng[z_cols].max(axis=1) - df_eng[z_cols].min(axis=1)
        
        # å¼‚å¸¸æŒ‡æ ‡ï¼ˆå¤šé˜ˆå€¼ï¼‰
        thresholds = [1.0, 1.5, 2.0, 2.5, 3.0, 3.5]
        for threshold in thresholds:
            df_eng[f'Zå€¼è¶…{threshold}_count'] = (df_eng[z_cols] > threshold).sum(axis=1)
            df_eng[f'Zå€¼è¶…{threshold}_æ¯”ä¾‹'] = df_eng[f'Zå€¼è¶…{threshold}_count'] / len(z_cols)
        
        # ç‰¹å®šæŸ“è‰²ä½“ç»„åˆå¼‚å¸¸
        df_eng['å¸¸æŸ“è‰²ä½“å¼‚å¸¸'] = (
            (df_eng['21å·æŸ“è‰²ä½“çš„Zå€¼ç»å¯¹å€¼'] > 2.0) |
            (df_eng['18å·æŸ“è‰²ä½“çš„Zå€¼ç»å¯¹å€¼'] > 2.0) |
            (df_eng['13å·æŸ“è‰²ä½“çš„Zå€¼ç»å¯¹å€¼'] > 2.0)
        ).astype(int)
        
        df_eng['XæŸ“è‰²ä½“å¼‚å¸¸'] = (df_eng['XæŸ“è‰²ä½“çš„Zå€¼ç»å¯¹å€¼'] > 2.0).astype(int)
        
        # 2. GCå«é‡ç‰¹å¾
        gc_cols = ['13å·æŸ“è‰²ä½“çš„GCå«é‡', '18å·æŸ“è‰²ä½“çš„GCå«é‡', '21å·æŸ“è‰²ä½“çš„GCå«é‡']
        
        df_eng['å¹³å‡GCå«é‡'] = df_eng[gc_cols].mean(axis=1)
        df_eng['GCæ ‡å‡†å·®'] = df_eng[gc_cols].std(axis=1)
        df_eng['GCåå·®'] = abs(df_eng['GCå«é‡'] - df_eng['å¹³å‡GCå«é‡'])
        
        # GCå«é‡å¼‚å¸¸æŒ‡æ ‡
        df_eng['GCå«é‡å¼‚å¸¸'] = (
            (df_eng['GCåå·®'] > df_eng['GCåå·®'].quantile(0.9)) |
            (df_eng['GCæ ‡å‡†å·®'] > df_eng['GCæ ‡å‡†å·®'].quantile(0.9))
        ).astype(int)
        
        # 3. XæŸ“è‰²ä½“ç‰¹å¼‚æ€§ç‰¹å¾
        df_eng['XæŸ“è‰²ä½“æµ“åº¦å¼‚å¸¸'] = (
            (df_eng['XæŸ“è‰²ä½“æµ“åº¦'] < 0.45) | (df_eng['XæŸ“è‰²ä½“æµ“åº¦'] > 0.55)
        ).astype(int)
        df_eng['XæŸ“è‰²ä½“æµ“åº¦åå·®'] = abs(df_eng['XæŸ“è‰²ä½“æµ“åº¦'] - 0.5)
        
        # 4. æµ‹åºè´¨é‡ç‰¹å¾
        df_eng['æµ‹åºæ·±åº¦å……è¶³'] = (df_eng['å”¯ä¸€æ¯”å¯¹çš„è¯»æ®µæ•°'] > 3000000).astype(int)
        df_eng['æ¯”å¯¹è´¨é‡è‰¯å¥½'] = (df_eng['åœ¨å‚è€ƒåŸºå› ç»„ä¸Šæ¯”å¯¹çš„æ¯”ä¾‹'] > 0.75).astype(int)
        df_eng['é‡å¤ç‡ä½'] = (df_eng['é‡å¤è¯»æ®µçš„æ¯”ä¾‹'] < 0.2).astype(int)
        
        # 5. ä¸´åºŠé£é™©å› å­
        df_eng['é«˜é¾„äº§å¦‡'] = (df_eng['å¹´é¾„'] >= 35).astype(int)
        df_eng['æé«˜é¾„äº§å¦‡'] = (df_eng['å¹´é¾„'] >= 40).astype(int)
        df_eng['BMIè¿‡é«˜'] = (df_eng['å­•å¦‡BMI'] > 30).astype(int)
        df_eng['BMIè‚¥èƒ–'] = (df_eng['å­•å¦‡BMI'] > 35).astype(int)
        
        # 6. äº¤äº’ç‰¹å¾
        df_eng['å¹´é¾„_Zå€¼äº¤äº’'] = df_eng['å¹´é¾„'] * df_eng['æœ€å¤§Zå€¼']
        df_eng['BMI_Zå€¼äº¤äº’'] = df_eng['å­•å¦‡BMI'] * df_eng['æœ€å¤§Zå€¼']
        df_eng['GC_Zå€¼äº¤äº’'] = df_eng['GCåå·®'] * df_eng['æœ€å¤§Zå€¼']
        
        # 7. å¤šé‡é£é™©è¯„åˆ†
        df_eng['ä¿å®ˆé£é™©è¯„åˆ†'] = (
            df_eng['æœ€å¤§Zå€¼'] * 0.4 +
            df_eng['Zå€¼è¶…2.5_count'] * 0.3 +
            df_eng['XæŸ“è‰²ä½“æµ“åº¦åå·®'] * 0.2 +
            df_eng['é«˜é¾„äº§å¦‡'] * 0.1
        )
        
        df_eng['å¹³è¡¡é£é™©è¯„åˆ†'] = (
            df_eng['æœ€å¤§Zå€¼'] * 0.3 +
            df_eng['Zå€¼è¶…2.0_count'] * 0.25 +
            df_eng['Zå€¼è¶…1.5_count'] * 0.2 +
            df_eng['GCåå·®'] * 0.15 +
            df_eng['é«˜é¾„äº§å¦‡'] * 0.1
        )
        
        df_eng['æ•æ„Ÿé£é™©è¯„åˆ†'] = (
            df_eng['æœ€å¤§Zå€¼'] * 0.25 +
            df_eng['Zå€¼è¶…1.5_count'] * 0.3 +
            df_eng['Zå€¼è¶…1.0_count'] * 0.25 +
            df_eng['GCåå·®'] * 0.1 +
            df_eng['é«˜é¾„äº§å¦‡'] * 0.05 +
            df_eng['BMIè¿‡é«˜'] * 0.05
        )
        
        # 8. å¤åˆå¼‚å¸¸æŒ‡æ ‡
        df_eng['å¤šé‡å¼‚å¸¸æŒ‡æ ‡'] = (
            df_eng['Zå€¼è¶…2.0_count'] +
            df_eng['GCå«é‡å¼‚å¸¸'] +
            df_eng['XæŸ“è‰²ä½“æµ“åº¦å¼‚å¸¸'] +
            df_eng['é«˜é¾„äº§å¦‡']
        )
        
        print(f"ç‰¹å¾å·¥ç¨‹å®Œæˆï¼Œç‰¹å¾æ•°é‡: {df_eng.shape[1]}")
        
        return df_eng
    
    def preprocess_features(self, df):
        """ç‰¹å¾é¢„å¤„ç†ï¼ˆç¼ºå¤±å€¼ã€ç‰¹å¾é€‰æ‹©ã€æ ‡å‡†åŒ–ï¼‰"""
        print("\n=== ç‰¹å¾é¢„å¤„ç† ===")
        
        if self.preprocessing_components is None:
            print("âŒ é¢„å¤„ç†ç»„ä»¶æœªåŠ è½½")
            return None, None
        
        # è·å–æ•°å€¼ç‰¹å¾
        numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()
        exclude_cols = ['åºå·', 'is_abnormal']
        feature_cols = [col for col in numeric_features if col not in exclude_cols]
        
        # ç¡®ä¿ä½¿ç”¨è®­ç»ƒæ—¶ç›¸åŒçš„ç‰¹å¾
        if 'feature_cols' in self.preprocessing_components:
            train_feature_cols = self.preprocessing_components['feature_cols']
            # åªä¿ç•™è®­ç»ƒæ—¶ä½¿ç”¨çš„ç‰¹å¾
            available_features = [col for col in train_feature_cols if col in df.columns]
            missing_features = [col for col in train_feature_cols if col not in df.columns]
            
            if missing_features:
                print(f"è­¦å‘Š: ç¼ºå°‘è®­ç»ƒæ—¶çš„ç‰¹å¾: {missing_features}")
            
            feature_cols = available_features
        
        print(f"ä½¿ç”¨ç‰¹å¾æ•°é‡: {len(feature_cols)}")
        
        X = df[feature_cols]
        
        # 1. ç¼ºå¤±å€¼å¤„ç†
        imputer = self.preprocessing_components['imputer']
        X_imputed = imputer.transform(X)
        
        # 2. ç‰¹å¾é€‰æ‹©
        selector = self.preprocessing_components['selector']
        X_selected = selector.transform(X_imputed)
        
        # 3. æ ‡å‡†åŒ–
        scaler = self.preprocessing_components['scaler']
        X_scaled = scaler.transform(X_selected)
        
        print(f"é¢„å¤„ç†åç‰¹å¾ç»´åº¦: {X_scaled.shape}")
        
        return X_scaled, feature_cols
    
    def predict_with_ensemble(self, X):
        """ä½¿ç”¨é›†æˆæ¨¡å‹è¿›è¡Œé¢„æµ‹"""
        print("\n=== é›†æˆæ¨¡å‹é¢„æµ‹ ===")
        
        if self.ensemble_model is None:
            print("âŒ é›†æˆæ¨¡å‹æœªåŠ è½½")
            return None, None, None
        
        if self.model_info is None:
            print("âŒ æ¨¡å‹ä¿¡æ¯æœªåŠ è½½")
            return None, None, None
        
        # è·å–æ¦‚ç‡é¢„æµ‹
        probabilities = self.ensemble_model.predict_proba(X)
        abnormal_probabilities = probabilities[:, 1]
        
        # ä½¿ç”¨ä¸åŒé˜ˆå€¼è¿›è¡Œé¢„æµ‹
        thresholds = self.model_info['optimal_thresholds']
        predictions = {}
        
        for scenario, threshold in thresholds.items():
            pred = (abnormal_probabilities > threshold).astype(int)
            predictions[scenario] = pred
            
            positive_count = pred.sum()
            positive_rate = positive_count / len(pred)
            print(f"{scenario}é˜ˆå€¼ ({threshold:.4f}): {positive_count} ä¸ªé˜³æ€§æ ·æœ¬ ({positive_rate:.1%})")
        
        # é»˜è®¤ä½¿ç”¨å¹³è¡¡é˜ˆå€¼
        default_predictions = predictions['balanced']
        
        return abnormal_probabilities, predictions, default_predictions
    
    def generate_risk_categories(self, probabilities):
        """ç”Ÿæˆé£é™©åˆ†çº§"""
        print("\n=== é£é™©åˆ†çº§ ===")
        
        risk_categories = []
        risk_levels = {'ä½é£é™©': 0, 'ä¸­é£é™©': 0, 'ä¸­-é«˜é£é™©': 0, 'é«˜é£é™©': 0}
        
        for prob in probabilities:
            if prob >= 0.8:
                category = 'é«˜é£é™©'
            elif prob >= 0.5:
                category = 'ä¸­-é«˜é£é™©'
            elif prob >= 0.2:
                category = 'ä¸­é£é™©'
            else:
                category = 'ä½é£é™©'
            
            risk_categories.append(category)
            risk_levels[category] += 1
        
        print("é£é™©åˆ†çº§ç»Ÿè®¡:")
        for level, count in risk_levels.items():
            print(f"  {level}: {count} ä¾‹ ({count/len(probabilities):.1%})")
        
        return risk_categories, risk_levels
    
    def create_prediction_report(self, df, probabilities, predictions, risk_categories, has_labels=False):
        """åˆ›å»ºé¢„æµ‹æŠ¥å‘Š"""
        print("\n=== ç”Ÿæˆé¢„æµ‹æŠ¥å‘Š ===")
        
        # åˆ›å»ºç»“æœDataFrame
        results_df = df.copy()
        results_df['å¼‚å¸¸æ¦‚ç‡'] = probabilities
        results_df['é£é™©ç­‰çº§'] = risk_categories
        
        # æ·»åŠ ä¸åŒé˜ˆå€¼ä¸‹çš„é¢„æµ‹ç»“æœ
        for scenario, pred in predictions.items():
            results_df[f'{scenario}_é¢„æµ‹'] = pred
            results_df[f'{scenario}_é¢„æµ‹ç»“æœ'] = ['å¼‚å¸¸' if p == 1 else 'æ­£å¸¸' for p in pred]
        
        # å¦‚æœæœ‰çœŸå®æ ‡ç­¾ï¼Œè®¡ç®—æ€§èƒ½æŒ‡æ ‡
        if has_labels:
            y_true = df['is_abnormal']
            
            print("\nğŸ“Š æ€§èƒ½è¯„ä¼°:")
            for scenario, pred in predictions.items():
                f1 = f1_score(y_true, pred)
                precision = precision_score(y_true, pred, zero_division=0)
                recall = recall_score(y_true, pred, zero_division=0)
                
                print(f"\n{scenario}é˜ˆå€¼:")
                print(f"  F1 Score: {f1:.4f}")
                print(f"  Precision: {precision:.4f}")
                print(f"  Recall: {recall:.4f}")
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        results_df.to_csv('ensemble_prediction_results.csv', index=False, encoding='utf-8-sig')
        print(f"\nè¯¦ç»†é¢„æµ‹ç»“æœå·²ä¿å­˜è‡³: ensemble_prediction_results.csv")
        
        # åˆ›å»ºæ±‡æ€»æŠ¥å‘Š
        summary_cols = ['åºå·', 'å­•å¦‡ä»£ç ', 'å¹´é¾„', 'å­•å¦‡BMI', 'æœ€å¤§Zå€¼', 'å¼‚å¸¸æ¦‚ç‡', 'é£é™©ç­‰çº§', 'balanced_é¢„æµ‹ç»“æœ']
        if has_labels:
            summary_cols.append('is_abnormal')
        
        summary_df = results_df[summary_cols]
        summary_df.to_csv('prediction_summary.csv', index=False, encoding='utf-8-sig')
        print(f"é¢„æµ‹æ±‡æ€»å·²ä¿å­˜è‡³: prediction_summary.csv")
        
        return results_df
    
    def create_visualizations(self, probabilities, risk_categories, predictions, has_labels=False, y_true=None):
        """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
        print("\n=== ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨ ===")
        
        if self.model_info is None:
            print("è­¦å‘Š: æ¨¡å‹ä¿¡æ¯æœªåŠ è½½ï¼Œä½¿ç”¨é»˜è®¤é˜ˆå€¼")
            default_thresholds = {'balanced': 0.5, 'high_recall': 0.3, 'high_precision': 0.7}
        else:
            default_thresholds = self.model_info['optimal_thresholds']
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        
        # 1. å¼‚å¸¸æ¦‚ç‡åˆ†å¸ƒ
        axes[0, 0].hist(probabilities, bins=30, alpha=0.7, color='skyblue', edgecolor='black')
        axes[0, 0].axvline(x=default_thresholds['balanced'], 
                          color='red', linestyle='--', label='å¹³è¡¡é˜ˆå€¼')
        axes[0, 0].set_xlabel('å¼‚å¸¸æ¦‚ç‡')
        axes[0, 0].set_ylabel('æ ·æœ¬æ•°é‡')
        axes[0, 0].set_title('å¼‚å¸¸æ¦‚ç‡åˆ†å¸ƒ')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. é£é™©ç­‰çº§åˆ†å¸ƒ
        risk_counts = pd.Series(risk_categories).value_counts()
        colors = ['green', 'yellow', 'orange', 'red']
        axes[0, 1].pie(risk_counts.values, labels=risk_counts.index, 
                            colors=colors, autopct='%1.1f%%', startangle=90)
        axes[0, 1].set_title('é£é™©ç­‰çº§åˆ†å¸ƒ')
        
        # 3. ä¸åŒé˜ˆå€¼ä¸‹çš„é¢„æµ‹ç»Ÿè®¡
        threshold_names = list(predictions.keys())
        positive_counts = [pred.sum() for pred in predictions.values()]
        
        bars = axes[0, 2].bar(threshold_names, positive_counts, 
                             color=['blue', 'green', 'red'], alpha=0.7)
        axes[0, 2].set_title('ä¸åŒé˜ˆå€¼ä¸‹çš„é˜³æ€§é¢„æµ‹æ•°')
        axes[0, 2].set_ylabel('é˜³æ€§æ ·æœ¬æ•°')
        axes[0, 2].grid(True, alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, count in zip(bars, positive_counts):
            height = bar.get_height()
            axes[0, 2].text(bar.get_x() + bar.get_width()/2., height + 0.5,
                           f'{count}', ha='center', va='bottom')
        
        # 4. é«˜é£é™©æ ·æœ¬çš„ç‰¹å¾åˆ†æ
        high_risk_mask = np.array(risk_categories) == 'é«˜é£é™©'
        if high_risk_mask.sum() > 0:
            high_risk_probs = probabilities[high_risk_mask]
            other_probs = probabilities[~high_risk_mask]
            
            axes[1, 0].boxplot([other_probs, high_risk_probs], 
                              labels=['å…¶ä»–', 'é«˜é£é™©'])
            axes[1, 0].set_title('é«˜é£é™© vs å…¶ä»–æ ·æœ¬çš„æ¦‚ç‡åˆ†å¸ƒ')
            axes[1, 0].set_ylabel('å¼‚å¸¸æ¦‚ç‡')
            axes[1, 0].grid(True, alpha=0.3)
        
        # 5. å¦‚æœæœ‰çœŸå®æ ‡ç­¾ï¼Œç»˜åˆ¶æ··æ·†çŸ©é˜µ
        if has_labels and y_true is not None:
            from sklearn.metrics import confusion_matrix
            
            cm = confusion_matrix(y_true, predictions['balanced'])
            axes[1, 1].imshow(cm, interpolation='nearest', cmap='Blues')
            axes[1, 1].set_title('æ··æ·†çŸ©é˜µ (å¹³è¡¡é˜ˆå€¼)')
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            thresh = cm.max() / 2.
            for i in range(cm.shape[0]):
                for j in range(cm.shape[1]):
                    axes[1, 1].text(j, i, format(cm[i, j], 'd'),
                                   ha="center", va="center",
                                   color="white" if cm[i, j] > thresh else "black")
            
            axes[1, 1].set_ylabel('çœŸå®æ ‡ç­¾')
            axes[1, 1].set_xlabel('é¢„æµ‹æ ‡ç­¾')
            axes[1, 1].set_xticks([0, 1])
            axes[1, 1].set_yticks([0, 1])
            axes[1, 1].set_xticklabels(['æ­£å¸¸', 'å¼‚å¸¸'])
            axes[1, 1].set_yticklabels(['æ­£å¸¸', 'å¼‚å¸¸'])
        
        # 6. æ¦‚ç‡æ ¡å‡†å›¾
        sorted_indices = np.argsort(probabilities)
        sorted_probs = probabilities[sorted_indices]
        
        axes[1, 2].plot(range(len(sorted_probs)), sorted_probs, 'b-', alpha=0.7)
        axes[1, 2].axhline(y=default_thresholds['balanced'], 
                          color='red', linestyle='--', label='å¹³è¡¡é˜ˆå€¼')
        axes[1, 2].axhline(y=default_thresholds['high_recall'], 
                          color='green', linestyle='--', label='é«˜å¬å›é˜ˆå€¼')
        axes[1, 2].axhline(y=default_thresholds['high_precision'], 
                          color='blue', linestyle='--', label='é«˜ç²¾ç¡®é˜ˆå€¼')
        axes[1, 2].set_xlabel('æ ·æœ¬ç´¢å¼• (æŒ‰æ¦‚ç‡æ’åº)')
        axes[1, 2].set_ylabel('å¼‚å¸¸æ¦‚ç‡')
        axes[1, 2].set_title('æ ·æœ¬æ¦‚ç‡åˆ†å¸ƒ (æ’åº)')
        axes[1, 2].legend()
        axes[1, 2].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('ensemble_prediction_analysis.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        return fig
    
    def run_prediction_pipeline(self, filepath='Q4/clean_girls_data_Q4.csv'):
        """è¿è¡Œå®Œæ•´çš„é¢„æµ‹æµæ°´çº¿"""
        print("ğŸš€ å¼€å§‹é›†æˆæ¨¡å‹é¢„æµ‹æµæ°´çº¿")
        print("="*60)
        
        # 1. åŠ è½½æ¨¡å‹
        if not self.load_ensemble_model():
            return None
        
        # 2. åŠ è½½å’Œé¢„å¤„ç†æ•°æ®
        processed_data, has_labels = self.load_and_preprocess_data(filepath)
        
        # 3. ç‰¹å¾é¢„å¤„ç†
        X, feature_cols = self.preprocess_features(processed_data)
        
        # 4. æ¨¡å‹é¢„æµ‹
        probabilities, predictions, default_pred = self.predict_with_ensemble(X)
        
        # 5. é£é™©åˆ†çº§
        risk_categories, risk_levels = self.generate_risk_categories(probabilities)
        
        # 6. ç”Ÿæˆé¢„æµ‹æŠ¥å‘Š
        results_df = self.create_prediction_report(
            processed_data, probabilities, predictions, risk_categories, has_labels
        )
        
        # 7. åˆ›å»ºå¯è§†åŒ–
        y_true = processed_data['is_abnormal'].values if has_labels else None
        self.create_visualizations(probabilities, risk_categories, predictions, has_labels, y_true)
        
        print("\nğŸ‰ é¢„æµ‹æµæ°´çº¿å®Œæˆï¼")
        print("ç”Ÿæˆçš„æ–‡ä»¶:")
        print("- ensemble_prediction_results.csv: è¯¦ç»†é¢„æµ‹ç»“æœ")
        print("- prediction_summary.csv: é¢„æµ‹æ±‡æ€»")
        print("- ensemble_prediction_analysis.png: å¯è§†åŒ–åˆ†æå›¾")
        
        return results_df, probabilities, predictions, risk_categories

if __name__ == "__main__":
    # åˆ›å»ºé¢„æµ‹ç³»ç»Ÿå¹¶è¿è¡Œ
    predictor = EnsemblePredictionSystem()
    
    # è¿è¡Œå®Œæ•´é¢„æµ‹æµæ°´çº¿
    results = predictor.run_prediction_pipeline('Q4/clean_girls_data_Q4.csv')
    
    if results is not None:
        results_df, probabilities, predictions, risk_categories = results
        print(f"\nâœ… é¢„æµ‹å®Œæˆï¼å…±å¤„ç† {len(results_df)} ä¸ªæ ·æœ¬")
        print(f"é«˜é£é™©æ ·æœ¬: {risk_categories.count('é«˜é£é™©')} ä¾‹")
        print(f"ä¸­-é«˜é£é™©æ ·æœ¬: {risk_categories.count('ä¸­-é«˜é£é™©')} ä¾‹")
    else:
        print("âŒ é¢„æµ‹å¤±è´¥")
