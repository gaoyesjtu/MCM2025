"""
æµ‹è¯•é›†æˆæ¨¡å‹åœ¨Q4æ•°æ®ä¸Šçš„æ€§èƒ½æŒ‡æ ‡
åŒ…æ‹¬Accuracyã€Precisionã€Recallã€AUCã€F1 scoreå’ŒNPV
"""

import pandas as pd
import numpy as np
import pickle
import json
from sklearn.metrics import (accuracy_score, precision_score, recall_score, 
                           f1_score, roc_auc_score, confusion_matrix)
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

class Q4PerformanceTester:
    """Q4æ•°æ®æ€§èƒ½æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.ensemble_model = None
        self.preprocessing_components = None
        self.model_info = None
        
    def load_models(self):
        """åŠ è½½æ¨¡å‹å’Œé¢„å¤„ç†ç»„ä»¶"""
        print("=== åŠ è½½æ¨¡å‹ç»„ä»¶ ===")
        
        try:
            # åŠ è½½é›†æˆæ¨¡å‹
            with open('ensemble_model.pkl', 'rb') as f:
                self.ensemble_model = pickle.load(f)
            print("âœ… é›†æˆæ¨¡å‹åŠ è½½æˆåŠŸ")
            
            # åŠ è½½é¢„å¤„ç†ç»„ä»¶
            with open('ensemble_preprocessing.pkl', 'rb') as f:
                self.preprocessing_components = pickle.load(f)
            print("âœ… é¢„å¤„ç†ç»„ä»¶åŠ è½½æˆåŠŸ")
            
            # åŠ è½½æ¨¡å‹ä¿¡æ¯
            with open('ensemble_model_info.json', 'r', encoding='utf-8') as f:
                self.model_info = json.load(f)
            print("âœ… æ¨¡å‹ä¿¡æ¯åŠ è½½æˆåŠŸ")
            
            return True
            
        except FileNotFoundError as e:
            print(f"âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {e}")
            return False
        except Exception as e:
            print(f"âŒ åŠ è½½æ¨¡å‹æ—¶å‡ºé”™: {e}")
            return False
    
    def advanced_feature_engineering(self, df):
        """é«˜çº§ç‰¹å¾å·¥ç¨‹ï¼ˆä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ï¼‰"""
        print("è¿›è¡Œç‰¹å¾å·¥ç¨‹...")
        
        df_eng = df.copy()
        
        # 1. Zå€¼ç›¸å…³ç‰¹å¾
        z_cols = ['XæŸ“è‰²ä½“çš„Zå€¼ç»å¯¹å€¼', '21å·æŸ“è‰²ä½“çš„Zå€¼ç»å¯¹å€¼', '18å·æŸ“è‰²ä½“çš„Zå€¼ç»å¯¹å€¼', '13å·æŸ“è‰²ä½“çš„Zå€¼ç»å¯¹å€¼']
        
        # åŸºç¡€ç»Ÿè®¡ç‰¹å¾
        df_eng['æœ€å¤§Zå€¼'] = df_eng[z_cols].max(axis=1)
        df_eng['å¹³å‡Zå€¼'] = df_eng[z_cols].mean(axis=1)
        df_eng['Zå€¼æ ‡å‡†å·®'] = df_eng[z_cols].std(axis=1)
        df_eng['Zå€¼å˜å¼‚ç³»æ•°'] = df_eng[z_cols].std(axis=1) / (df_eng[z_cols].mean(axis=1) + 1e-8)
        df_eng['Zå€¼èŒƒå›´'] = df_eng[z_cols].max(axis=1) - df_eng[z_cols].min(axis=1)
        
        # å¼‚å¸¸æŒ‡æ ‡ï¼ˆå¤šé˜ˆå€¼ï¼‰
        thresholds = [1.0, 1.5, 2.0, 2.5, 3.0, 3.5]
        for threshold in thresholds:
            df_eng[f'Zå€¼è¶…{threshold}_count'] = (df_eng[z_cols] > threshold).sum(axis=1)
            df_eng[f'Zå€¼è¶…{threshold}_æ¯”ä¾‹'] = df_eng[f'Zå€¼è¶…{threshold}_count'] / len(z_cols)
        
        # ç‰¹å®šæŸ“è‰²ä½“ç»„åˆå¼‚å¸¸
        df_eng['å¸¸æŸ“è‰²ä½“å¼‚å¸¸'] = (
            (df_eng['21å·æŸ“è‰²ä½“çš„Zå€¼ç»å¯¹å€¼'] > 2.0) |
            (df_eng['18å·æŸ“è‰²ä½“çš„Zå€¼ç»å¯¹å€¼'] > 2.0) |
            (df_eng['13å·æŸ“è‰²ä½“çš„Zå€¼ç»å¯¹å€¼'] > 2.0)
        ).astype(int)
        
        df_eng['XæŸ“è‰²ä½“å¼‚å¸¸'] = (df_eng['XæŸ“è‰²ä½“çš„Zå€¼ç»å¯¹å€¼'] > 2.0).astype(int)
        
        # 2. GCå«é‡ç‰¹å¾
        gc_cols = ['13å·æŸ“è‰²ä½“çš„GCå«é‡', '18å·æŸ“è‰²ä½“çš„GCå«é‡', '21å·æŸ“è‰²ä½“çš„GCå«é‡']
        
        df_eng['å¹³å‡GCå«é‡'] = df_eng[gc_cols].mean(axis=1)
        df_eng['GCæ ‡å‡†å·®'] = df_eng[gc_cols].std(axis=1)
        df_eng['GCåå·®'] = abs(df_eng['GCå«é‡'] - df_eng['å¹³å‡GCå«é‡'])
        
        # GCå«é‡å¼‚å¸¸æŒ‡æ ‡
        df_eng['GCå«é‡å¼‚å¸¸'] = (
            (df_eng['GCåå·®'] > df_eng['GCåå·®'].quantile(0.9)) |
            (df_eng['GCæ ‡å‡†å·®'] > df_eng['GCæ ‡å‡†å·®'].quantile(0.9))
        ).astype(int)
        
        # 3. XæŸ“è‰²ä½“ç‰¹å¼‚æ€§ç‰¹å¾
        df_eng['XæŸ“è‰²ä½“æµ“åº¦å¼‚å¸¸'] = (
            (df_eng['XæŸ“è‰²ä½“æµ“åº¦'] < 0.45) | (df_eng['XæŸ“è‰²ä½“æµ“åº¦'] > 0.55)
        ).astype(int)
        df_eng['XæŸ“è‰²ä½“æµ“åº¦åå·®'] = abs(df_eng['XæŸ“è‰²ä½“æµ“åº¦'] - 0.5)
        
        # 4. æµ‹åºè´¨é‡ç‰¹å¾
        df_eng['æµ‹åºæ·±åº¦å……è¶³'] = (df_eng['å”¯ä¸€æ¯”å¯¹çš„è¯»æ®µæ•°'] > 3000000).astype(int)
        df_eng['æ¯”å¯¹è´¨é‡è‰¯å¥½'] = (df_eng['åœ¨å‚è€ƒåŸºå› ç»„ä¸Šæ¯”å¯¹çš„æ¯”ä¾‹'] > 0.75).astype(int)
        df_eng['é‡å¤ç‡ä½'] = (df_eng['é‡å¤è¯»æ®µçš„æ¯”ä¾‹'] < 0.2).astype(int)
        
        # 5. ä¸´åºŠé£é™©å› å­
        df_eng['é«˜é¾„äº§å¦‡'] = (df_eng['å¹´é¾„'] >= 35).astype(int)
        df_eng['æé«˜é¾„äº§å¦‡'] = (df_eng['å¹´é¾„'] >= 40).astype(int)
        df_eng['BMIè¿‡é«˜'] = (df_eng['å­•å¦‡BMI'] > 30).astype(int)
        df_eng['BMIè‚¥èƒ–'] = (df_eng['å­•å¦‡BMI'] > 35).astype(int)
        
        # 6. äº¤äº’ç‰¹å¾
        df_eng['å¹´é¾„_Zå€¼äº¤äº’'] = df_eng['å¹´é¾„'] * df_eng['æœ€å¤§Zå€¼']
        df_eng['BMI_Zå€¼äº¤äº’'] = df_eng['å­•å¦‡BMI'] * df_eng['æœ€å¤§Zå€¼']
        df_eng['GC_Zå€¼äº¤äº’'] = df_eng['GCåå·®'] * df_eng['æœ€å¤§Zå€¼']
        
        # 7. å¤šé‡é£é™©è¯„åˆ†
        df_eng['ä¿å®ˆé£é™©è¯„åˆ†'] = (
            df_eng['æœ€å¤§Zå€¼'] * 0.4 +
            df_eng['Zå€¼è¶…2.5_count'] * 0.3 +
            df_eng['XæŸ“è‰²ä½“æµ“åº¦åå·®'] * 0.2 +
            df_eng['é«˜é¾„äº§å¦‡'] * 0.1
        )
        
        df_eng['å¹³è¡¡é£é™©è¯„åˆ†'] = (
            df_eng['æœ€å¤§Zå€¼'] * 0.3 +
            df_eng['Zå€¼è¶…2.0_count'] * 0.25 +
            df_eng['Zå€¼è¶…1.5_count'] * 0.2 +
            df_eng['GCåå·®'] * 0.15 +
            df_eng['é«˜é¾„äº§å¦‡'] * 0.1
        )
        
        df_eng['æ•æ„Ÿé£é™©è¯„åˆ†'] = (
            df_eng['æœ€å¤§Zå€¼'] * 0.25 +
            df_eng['Zå€¼è¶…1.5_count'] * 0.3 +
            df_eng['Zå€¼è¶…1.0_count'] * 0.25 +
            df_eng['GCåå·®'] * 0.1 +
            df_eng['é«˜é¾„äº§å¦‡'] * 0.05 +
            df_eng['BMIè¿‡é«˜'] * 0.05
        )
        
        # 8. å¤åˆå¼‚å¸¸æŒ‡æ ‡
        df_eng['å¤šé‡å¼‚å¸¸æŒ‡æ ‡'] = (
            df_eng['Zå€¼è¶…2.0_count'] +
            df_eng['GCå«é‡å¼‚å¸¸'] +
            df_eng['XæŸ“è‰²ä½“æµ“åº¦å¼‚å¸¸'] +
            df_eng['é«˜é¾„äº§å¦‡']
        )
        
        print(f"ç‰¹å¾å·¥ç¨‹å®Œæˆï¼Œç‰¹å¾æ•°é‡: {df_eng.shape[1]}")
        
        return df_eng
    
    def preprocess_features(self, df):
        """ç‰¹å¾é¢„å¤„ç†"""
        print("è¿›è¡Œç‰¹å¾é¢„å¤„ç†...")
        
        # è·å–æ•°å€¼ç‰¹å¾
        numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()
        exclude_cols = ['åºå·', 'is_abnormal']
        feature_cols = [col for col in numeric_features if col not in exclude_cols]
        
        # ç¡®ä¿ä½¿ç”¨è®­ç»ƒæ—¶ç›¸åŒçš„ç‰¹å¾
        if 'feature_cols' in self.preprocessing_components:
            train_feature_cols = self.preprocessing_components['feature_cols']
            available_features = [col for col in train_feature_cols if col in df.columns]
            feature_cols = available_features
        
        X = df[feature_cols]
        
        # 1. ç¼ºå¤±å€¼å¤„ç†
        imputer = self.preprocessing_components['imputer']
        X_imputed = imputer.transform(X)
        
        # 2. ç‰¹å¾é€‰æ‹©
        selector = self.preprocessing_components['selector']
        X_selected = selector.transform(X_imputed)
        
        # 3. æ ‡å‡†åŒ–
        scaler = self.preprocessing_components['scaler']
        X_scaled = scaler.transform(X_selected)
        
        return X_scaled
    
    def calculate_comprehensive_metrics(self, y_true, y_pred, y_prob, threshold_name="balanced"):
        """è®¡ç®—å…¨é¢çš„æ€§èƒ½æŒ‡æ ‡"""
        print(f"\n=== {threshold_name}é˜ˆå€¼æ€§èƒ½æŒ‡æ ‡ ===")
        
        # åŸºç¡€æŒ‡æ ‡
        accuracy = accuracy_score(y_true, y_pred)
        precision = precision_score(y_true, y_pred, zero_division=0)
        recall = recall_score(y_true, y_pred, zero_division=0)
        f1 = f1_score(y_true, y_pred, zero_division=0)
        
        # è®¡ç®—æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(y_true, y_pred)
        tn, fp, fn, tp = cm.ravel()
        
        # è®¡ç®—ç‰¹å¼‚æ€§å’ŒNPV
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
        npv = tn / (tn + fn) if (tn + fn) > 0 else 0  # é˜´æ€§é¢„æµ‹å€¼
        
        # è®¡ç®—AUC
        try:
            auc = roc_auc_score(y_true, y_prob)
        except Exception as e:
            auc = 0.0
            print(f"è­¦å‘Š: æ— æ³•è®¡ç®—AUC: {e}")
        
        # æ‰“å°ç»“æœ
        print(f"ğŸ¯ æ ¸å¿ƒæŒ‡æ ‡:")
        print(f"   Accuracy (å‡†ç¡®ç‡):     {accuracy:.4f} ({accuracy:.1%})")
        print(f"   Precision (ç²¾ç¡®ç‡):    {precision:.4f} ({precision:.1%})")
        print(f"   Recall (å¬å›ç‡):       {recall:.4f} ({recall:.1%})")
        print(f"   F1 Score:             {f1:.4f}")
        print(f"   AUC:                  {auc:.4f}")
        print(f"   NPV (é˜´æ€§é¢„æµ‹å€¼):      {npv:.4f} ({npv:.1%})")
        print(f"   Specificity (ç‰¹å¼‚æ€§): {specificity:.4f} ({specificity:.1%})")
        
        print(f"\nğŸ“Š æ··æ·†çŸ©é˜µ:")
        print(f"                å®é™…")
        print(f"é¢„æµ‹    æ­£å¸¸    å¼‚å¸¸")
        print(f"æ­£å¸¸     {tn:3d}     {fn:3d}")
        print(f"å¼‚å¸¸     {fp:3d}     {tp:3d}")
        
        # è¿”å›æŒ‡æ ‡å­—å…¸
        metrics = {
            'threshold_name': threshold_name,
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'specificity': specificity,
            'f1_score': f1,
            'auc': auc,
            'npv': npv,
            'confusion_matrix': {'tn': tn, 'fp': fp, 'fn': fn, 'tp': tp}
        }
        
        return metrics
    
    def test_q4_performance(self, filepath='Q4/clean_girls_data_Q4.csv'):
        """æµ‹è¯•Q4æ•°æ®æ€§èƒ½"""
        print("ğŸš€ å¼€å§‹Q4æ•°æ®æ€§èƒ½æµ‹è¯•")
        print("="*60)
        
        # 1. åŠ è½½æ¨¡å‹
        if not self.load_models():
            return None
        
        # 2. åŠ è½½Q4æ•°æ®
        print(f"\n=== åŠ è½½Q4æ•°æ®: {filepath} ===")
        data = pd.read_csv(filepath)
        print(f"æ•°æ®é›†å¤§å°: {data.shape}")
        
        # æ£€æŸ¥æ ‡ç­¾åˆ†å¸ƒ
        if 'is_abnormal' not in data.columns:
            print("âŒ Q4æ•°æ®ä¸­æœªæ‰¾åˆ°çœŸå®æ ‡ç­¾")
            return None
        
        y_true = data['is_abnormal']
        print(f"æ ‡ç­¾åˆ†å¸ƒ: æ­£å¸¸ {(y_true==0).sum()} | å¼‚å¸¸ {(y_true==1).sum()}")
        print(f"å¼‚å¸¸ç‡: {y_true.mean():.1%}")
        
        # 3. ç‰¹å¾å·¥ç¨‹
        processed_data = self.advanced_feature_engineering(data)
        
        # 4. ç‰¹å¾é¢„å¤„ç†
        X = self.preprocess_features(processed_data)
        print(f"é¢„å¤„ç†åç‰¹å¾ç»´åº¦: {X.shape}")
        
        # 5. æ¨¡å‹é¢„æµ‹
        print(f"\n=== æ¨¡å‹é¢„æµ‹ ===")
        probabilities = self.ensemble_model.predict_proba(X)[:, 1]
        
        # 6. ä½¿ç”¨ä¸åŒé˜ˆå€¼è¿›è¡Œé¢„æµ‹å’Œè¯„ä¼°
        thresholds = self.model_info['optimal_thresholds']
        all_metrics = {}
        predictions_dict = {}
        
        for scenario, threshold in thresholds.items():
            y_pred = (probabilities > threshold).astype(int)
            predictions_dict[scenario] = y_pred
            metrics = self.calculate_comprehensive_metrics(y_true, y_pred, probabilities, scenario)
            all_metrics[scenario] = metrics
        
        # 6.5. ç”Ÿæˆé£é™©åˆ†çº§
        risk_categories = self.generate_risk_categories(probabilities)
        
        # 6.6. åˆ›å»ºè¯¦ç»†é¢„æµ‹ç»“æœCSV
        self.create_prediction_results_csv(data, processed_data, probabilities, predictions_dict, risk_categories, y_true)
        
        # 7. åˆ›å»ºæ€§èƒ½å¯¹æ¯”è¡¨
        print(f"\n" + "="*60)
        print("ğŸ“ˆ æ€§èƒ½å¯¹æ¯”æ±‡æ€»")
        print("="*60)
        
        metrics_df = pd.DataFrame(all_metrics).T
        display_cols = ['accuracy', 'precision', 'recall', 'specificity', 'f1_score', 'auc', 'npv']
        
        print("\næŒ‡æ ‡å¯¹æ¯”è¡¨:")
        print(metrics_df[display_cols].round(4))
        
        # 8. ä¿å­˜ç»“æœ
        metrics_df.to_csv('Q4_performance_metrics.csv', encoding='utf-8-sig')
        print(f"\nğŸ“ æ€§èƒ½æŒ‡æ ‡å·²ä¿å­˜è‡³: Q4_performance_metrics.csv")
        
        # 9. åˆ›å»ºå¯è§†åŒ–
        self.create_performance_visualization(all_metrics, y_true, probabilities)
        
        print("\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
        print("- Q4_performance_metrics.csv: æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”")
        print("- Q4_detailed_prediction_results.csv: è¯¦ç»†é¢„æµ‹ç»“æœ")
        print("- Q4_prediction_summary.csv: é¢„æµ‹æ±‡æ€»")
        print("- Q4_false_positive_cases.csv: å‡é˜³æ€§æ¡ˆä¾‹åˆ†æ")
        print("- Q4_false_negative_cases.csv: å‡é˜´æ€§æ¡ˆä¾‹åˆ†æ")
        print("- Q4_performance_analysis.png: æ€§èƒ½å¯è§†åŒ–å›¾è¡¨")
        
        return all_metrics, metrics_df
    
    def generate_risk_categories(self, probabilities):
        """ç”Ÿæˆé£é™©åˆ†çº§"""
        print("\n=== é£é™©åˆ†çº§ ===")
        
        risk_categories = []
        risk_levels = {'ä½é£é™©': 0, 'ä¸­é£é™©': 0, 'ä¸­-é«˜é£é™©': 0, 'é«˜é£é™©': 0}
        
        for prob in probabilities:
            if prob >= 0.8:
                category = 'é«˜é£é™©'
            elif prob >= 0.5:
                category = 'ä¸­-é«˜é£é™©'
            elif prob >= 0.2:
                category = 'ä¸­é£é™©'
            else:
                category = 'ä½é£é™©'
            
            risk_categories.append(category)
            risk_levels[category] += 1
        
        print("é£é™©åˆ†çº§ç»Ÿè®¡:")
        for level, count in risk_levels.items():
            print(f"  {level}: {count} ä¾‹ ({count/len(probabilities):.1%})")
        
        return risk_categories
    
    def create_prediction_results_csv(self, original_data, processed_data, probabilities, predictions_dict, risk_categories, y_true):
        """åˆ›å»ºè¯¦ç»†çš„é¢„æµ‹ç»“æœCSVæ–‡ä»¶"""
        print("\n=== ç”Ÿæˆé¢„æµ‹ç»“æœCSV ===")
        
        # åˆ›å»ºç»“æœDataFrameï¼Œä»åŸå§‹æ•°æ®å¼€å§‹
        results_df = original_data.copy()
        
        # æ·»åŠ é¢„æµ‹ç›¸å…³åˆ—
        results_df['å¼‚å¸¸æ¦‚ç‡'] = probabilities
        results_df['é£é™©ç­‰çº§'] = risk_categories
        
        # æ·»åŠ ä¸åŒé˜ˆå€¼ä¸‹çš„é¢„æµ‹ç»“æœ
        for scenario, predictions in predictions_dict.items():
            results_df[f'{scenario}_é¢„æµ‹å€¼'] = predictions
            results_df[f'{scenario}_é¢„æµ‹ç»“æœ'] = ['å¼‚å¸¸' if p == 1 else 'æ­£å¸¸' for p in predictions]
        
        # æ·»åŠ ä¸€äº›å…³é”®çš„å·¥ç¨‹ç‰¹å¾
        if 'æœ€å¤§Zå€¼' in processed_data.columns:
            results_df['æœ€å¤§Zå€¼'] = processed_data['æœ€å¤§Zå€¼']
        if 'Zå€¼è¶…2.0_count' in processed_data.columns:
            results_df['Zå€¼è¶…2.0_count'] = processed_data['Zå€¼è¶…2.0_count']
        if 'é«˜é¾„äº§å¦‡' in processed_data.columns:
            results_df['é«˜é¾„äº§å¦‡'] = processed_data['é«˜é¾„äº§å¦‡']
        
        # æ·»åŠ é¢„æµ‹å‡†ç¡®æ€§åˆ†æï¼ˆé’ˆå¯¹æ¯ä¸ªé˜ˆå€¼ï¼‰
        for scenario in predictions_dict.keys():
            y_pred = predictions_dict[scenario]
            results_df[f'{scenario}_é¢„æµ‹æ­£ç¡®'] = (y_true == y_pred).astype(int)
            results_df[f'{scenario}_é¢„æµ‹ç±»å‹'] = ''
            
            # æ ‡è®°é¢„æµ‹ç±»å‹
            for i in range(len(y_true)):
                if y_true.iloc[i] == 1 and y_pred[i] == 1:
                    results_df.loc[i, f'{scenario}_é¢„æµ‹ç±»å‹'] = 'TP'
                elif y_true.iloc[i] == 0 and y_pred[i] == 0:
                    results_df.loc[i, f'{scenario}_é¢„æµ‹ç±»å‹'] = 'TN'
                elif y_true.iloc[i] == 0 and y_pred[i] == 1:
                    results_df.loc[i, f'{scenario}_é¢„æµ‹ç±»å‹'] = 'FP'
                else:  # y_true == 1 and y_pred == 0
                    results_df.loc[i, f'{scenario}_é¢„æµ‹ç±»å‹'] = 'FN'
        
        # ä¿å­˜è¯¦ç»†é¢„æµ‹ç»“æœ
        results_df.to_csv('Q4_detailed_prediction_results.csv', index=False, encoding='utf-8-sig')
        print("è¯¦ç»†é¢„æµ‹ç»“æœå·²ä¿å­˜è‡³: Q4_detailed_prediction_results.csv")
        
        # åˆ›å»ºç®€åŒ–çš„æ±‡æ€»è¡¨
        summary_cols = ['åºå·', 'å­•å¦‡ä»£ç ', 'å¹´é¾„', 'å­•å¦‡BMI', 'å¼‚å¸¸æ¦‚ç‡', 'é£é™©ç­‰çº§', 
                       'balanced_é¢„æµ‹ç»“æœ', 'is_abnormal', 'balanced_é¢„æµ‹æ­£ç¡®', 'balanced_é¢„æµ‹ç±»å‹']
        
        # æ·»åŠ æœ€å¤§Zå€¼ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if 'æœ€å¤§Zå€¼' in results_df.columns:
            summary_cols.insert(-4, 'æœ€å¤§Zå€¼')
        
        summary_df = results_df[summary_cols]
        summary_df.to_csv('Q4_prediction_summary.csv', index=False, encoding='utf-8-sig')
        print("é¢„æµ‹æ±‡æ€»å·²ä¿å­˜è‡³: Q4_prediction_summary.csv")
        
        # åˆ›å»ºé”™è¯¯åˆ†ææŠ¥å‘Š
        self.create_error_analysis_report(results_df, y_true, predictions_dict)
        
        return results_df
    
    def create_error_analysis_report(self, results_df, y_true, predictions_dict):
        """åˆ›å»ºé”™è¯¯åˆ†ææŠ¥å‘Š"""
        print("\n=== é”™è¯¯åˆ†æ ===")
        
        error_analysis = {}
        
        for scenario in predictions_dict.keys():
            y_pred = predictions_dict[scenario]
            
            # å‡é˜³æ€§åˆ†æ
            fp_mask = (y_true == 0) & (y_pred == 1)
            fp_cases = results_df[fp_mask]
            
            # å‡é˜´æ€§åˆ†æ  
            fn_mask = (y_true == 1) & (y_pred == 0)
            fn_cases = results_df[fn_mask]
            
            error_analysis[scenario] = {
                'false_positive_count': fp_mask.sum(),
                'false_negative_count': fn_mask.sum(),
                'fp_avg_probability': results_df.loc[fp_mask, 'å¼‚å¸¸æ¦‚ç‡'].mean() if fp_mask.sum() > 0 else 0,
                'fn_avg_probability': results_df.loc[fn_mask, 'å¼‚å¸¸æ¦‚ç‡'].mean() if fn_mask.sum() > 0 else 0,
            }
            
            print(f"\n{scenario}é˜ˆå€¼é”™è¯¯åˆ†æ:")
            print(f"  å‡é˜³æ€§: {fp_mask.sum()} ä¾‹, å¹³å‡æ¦‚ç‡: {error_analysis[scenario]['fp_avg_probability']:.4f}")
            print(f"  å‡é˜´æ€§: {fn_mask.sum()} ä¾‹, å¹³å‡æ¦‚ç‡: {error_analysis[scenario]['fn_avg_probability']:.4f}")
        
        # ä¿å­˜é”™è¯¯æ¡ˆä¾‹è¯¦æƒ…
        if 'balanced' in predictions_dict:
            balanced_pred = predictions_dict['balanced']
            fp_mask = (y_true == 0) & (balanced_pred == 1)
            fn_mask = (y_true == 1) & (balanced_pred == 0)
            
            if fp_mask.sum() > 0:
                fp_cases = results_df[fp_mask][['åºå·', 'å­•å¦‡ä»£ç ', 'å¹´é¾„', 'å­•å¦‡BMI', 'å¼‚å¸¸æ¦‚ç‡', 'é£é™©ç­‰çº§', 'is_abnormal']]
                fp_cases.to_csv('Q4_false_positive_cases.csv', index=False, encoding='utf-8-sig')
                print(f"å‡é˜³æ€§æ¡ˆä¾‹å·²ä¿å­˜è‡³: Q4_false_positive_cases.csv ({fp_mask.sum()} ä¾‹)")
                
            if fn_mask.sum() > 0:
                fn_cases = results_df[fn_mask][['åºå·', 'å­•å¦‡ä»£ç ', 'å¹´é¾„', 'å­•å¦‡BMI', 'å¼‚å¸¸æ¦‚ç‡', 'é£é™©ç­‰çº§', 'is_abnormal']]
                fn_cases.to_csv('Q4_false_negative_cases.csv', index=False, encoding='utf-8-sig')
                print(f"å‡é˜´æ€§æ¡ˆä¾‹å·²ä¿å­˜è‡³: Q4_false_negative_cases.csv ({fn_mask.sum()} ä¾‹)")
        
    def create_performance_visualization(self, all_metrics, y_true, probabilities):
        """åˆ›å»ºæ€§èƒ½å¯è§†åŒ–å›¾è¡¨"""
        print(f"\n=== ç”Ÿæˆæ€§èƒ½å¯è§†åŒ– ===")
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # 1. æŒ‡æ ‡å¯¹æ¯”é›·è¾¾å›¾
        scenarios = list(all_metrics.keys())
        metrics_names = ['accuracy', 'precision', 'recall', 'specificity', 'f1_score', 'auc', 'npv']
        
        # å‡†å¤‡é›·è¾¾å›¾æ•°æ®
        angles = np.linspace(0, 2*np.pi, len(metrics_names), endpoint=False).tolist()
        angles += angles[:1]  # é—­åˆå›¾å½¢
        
        ax1 = plt.subplot(2, 2, 1, projection='polar')
        colors = ['blue', 'green', 'red']
        
        for i, scenario in enumerate(scenarios):
            values = [all_metrics[scenario][metric] for metric in metrics_names]
            values += values[:1]  # é—­åˆå›¾å½¢
            
            ax1.plot(angles, values, 'o-', linewidth=2, label=scenario, color=colors[i])
            ax1.fill(angles, values, alpha=0.25, color=colors[i])
        
        ax1.set_xticks(angles[:-1])
        ax1.set_xticklabels(['å‡†ç¡®ç‡', 'ç²¾ç¡®ç‡', 'å¬å›ç‡', 'ç‰¹å¼‚æ€§', 'F1', 'AUC', 'NPV'])
        ax1.set_ylim(0, 1)
        ax1.set_title('æ€§èƒ½æŒ‡æ ‡é›·è¾¾å›¾', pad=20)
        ax1.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
        
        # 2. ROCæ›²çº¿ï¼ˆå¦‚æœå¯èƒ½ï¼‰
        try:
            from sklearn.metrics import roc_curve
            fpr, tpr, _ = roc_curve(y_true, probabilities)
            auc_score = all_metrics['balanced']['auc']
            
            axes[0, 1].plot(fpr, tpr, 'b-', label=f'ROCæ›²çº¿ (AUC = {auc_score:.4f})')
            axes[0, 1].plot([0, 1], [0, 1], 'r--', label='éšæœºåˆ†ç±»å™¨')
            axes[0, 1].set_xlabel('å‡é˜³æ€§ç‡ (1-ç‰¹å¼‚æ€§)')
            axes[0, 1].set_ylabel('çœŸé˜³æ€§ç‡ (æ•æ„Ÿæ€§)')
            axes[0, 1].set_title('ROCæ›²çº¿')
            axes[0, 1].legend()
            axes[0, 1].grid(True, alpha=0.3)
        except Exception as e:
            axes[0, 1].text(0.5, 0.5, f'æ— æ³•ç»˜åˆ¶ROCæ›²çº¿\n{str(e)}', 
                           ha='center', va='center', transform=axes[0, 1].transAxes)
            axes[0, 1].set_title('ROCæ›²çº¿ (ç»˜åˆ¶å¤±è´¥)')
        
        # 3. æ¦‚ç‡åˆ†å¸ƒç›´æ–¹å›¾
        normal_probs = probabilities[y_true == 0]
        abnormal_probs = probabilities[y_true == 1]
        
        axes[1, 0].hist(normal_probs, bins=30, alpha=0.7, color='green', label='æ­£å¸¸æ ·æœ¬', density=True)
        axes[1, 0].hist(abnormal_probs, bins=30, alpha=0.7, color='red', label='å¼‚å¸¸æ ·æœ¬', density=True)
        axes[1, 0].axvline(x=0.5, color='black', linestyle='--', label='é˜ˆå€¼=0.5')
        axes[1, 0].set_xlabel('å¼‚å¸¸æ¦‚ç‡')
        axes[1, 0].set_ylabel('å¯†åº¦')
        axes[1, 0].set_title('å¼‚å¸¸æ¦‚ç‡åˆ†å¸ƒ')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        # 4. æ··æ·†çŸ©é˜µçƒ­åŠ›å›¾
        balanced_cm = np.array([[all_metrics['balanced']['confusion_matrix']['tn'], 
                                all_metrics['balanced']['confusion_matrix']['fn']],
                               [all_metrics['balanced']['confusion_matrix']['fp'], 
                                all_metrics['balanced']['confusion_matrix']['tp']]])
        
        sns.heatmap(balanced_cm, annot=True, fmt='d', cmap='Blues', 
                   xticklabels=['æ­£å¸¸', 'å¼‚å¸¸'], yticklabels=['æ­£å¸¸', 'å¼‚å¸¸'],
                   ax=axes[1, 1])
        axes[1, 1].set_title('æ··æ·†çŸ©é˜µ (å¹³è¡¡é˜ˆå€¼)')
        axes[1, 1].set_xlabel('é¢„æµ‹æ ‡ç­¾')
        axes[1, 1].set_ylabel('çœŸå®æ ‡ç­¾')
        
        plt.tight_layout()
        plt.savefig('Q4_performance_analysis.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print("ğŸ“Š æ€§èƒ½å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜è‡³: Q4_performance_analysis.png")

if __name__ == "__main__":
    # åˆ›å»ºæµ‹è¯•å™¨å¹¶è¿è¡Œ
    tester = Q4PerformanceTester()
    
    # æµ‹è¯•Q4æ•°æ®æ€§èƒ½
    results = tester.test_q4_performance('Q4/clean_girls_data_Q4.csv')
    
    if results is not None:
        all_metrics, metrics_df = results
        print(f"\nâœ… Q4æ€§èƒ½æµ‹è¯•å®Œæˆï¼")
        print(f"\nğŸ¯ æ¨èä½¿ç”¨å¹³è¡¡é˜ˆå€¼ï¼Œä¸»è¦æŒ‡æ ‡:")
        balanced = all_metrics['balanced']
        print(f"   å‡†ç¡®ç‡ (Accuracy): {balanced['accuracy']:.4f}")
        print(f"   ç²¾ç¡®ç‡ (Precision): {balanced['precision']:.4f}")
        print(f"   å¬å›ç‡ (Recall): {balanced['recall']:.4f}")
        print(f"   F1åˆ†æ•°: {balanced['f1_score']:.4f}")
        print(f"   AUC: {balanced['auc']:.4f}")
        print(f"   é˜´æ€§é¢„æµ‹å€¼ (NPV): {balanced['npv']:.4f}")
    else:
        print("âŒ Q4æ€§èƒ½æµ‹è¯•å¤±è´¥")
